# AI机械臂智能体项目分析文档

## 项目概述

这是一个基于AI的机械臂智能体项目，能够实现听懂语音指令、看懂图像、执行相应动作的功能。项目结合了大语言模型、多模态视觉模型、语音识别、语音合成等技术，构建了一个具备感知、理解和执行能力的具身智能体。

### 项目特点

1. **多模态交互**：支持语音、图像等多种输入方式
2. **智能规划**：通过大语言模型理解用户意图，规划机械臂动作序列
3. **视觉定位**：利用多模态大模型识别图像中的物体，实现精确定位
4. **灵活执行**：支持多种机械臂动作，如归零、关节控制、坐标移动等
5. **拖动示教**：支持用户手动拖动机械臂，记录并复现动作

## 文件结构与功能说明

### 主程序与配置文件

#### agent_go.py
- **功能**：项目主入口，负责初始化系统、接收用户指令、协调各模块工作
- **主要流程**：
  1. 导入所有工具模块
  2. 初始化系统（关闭吸泵、播放欢迎词）
  3. 接收用户指令（语音或键盘输入）
  4. 调用智能体规划动作
  5. 执行规划的动作序列
  6. 语音反馈执行结果

#### API_KEY.py
- **功能**：存储各种API密钥，包括零一万物、通义千问、百度智能云等大模型平台的API密钥
- **注意事项**：该文件包含敏感信息，不应公开分享

### 智能体与大模型相关

#### utils_agent.py
- **功能**：定义智能体系统提示词，根据用户指令规划机械臂动作
- **核心内容**：
  - 定义了AGENT_SYS_PROMPT系统提示词，包含所有机械臂可执行的函数列表
  - 提供agent_plan函数，调用大语言模型生成动作序列

#### utils_llm.py
- **功能**：封装大语言模型API调用
- **支持模型**：
  - 百度智能云千帆大模型（ERNIE-Bot-4等）
  - 零一万物大模型（yi-large等）

#### utils_vlm.py
- **功能**：封装多模态大模型API调用，实现图像识别和理解
- **支持模型**：
  - 零一万物yi-vision模型
  - 通义千问QwenVL模型
- **核心功能**：
  - 物体定位：识别图像中指定物体的位置
  - 视觉问答：回答关于图像内容的问题
  - 结果可视化：在图像上标记识别结果

### 机械臂控制相关

#### utils_robot.py
- **功能**：机械臂运动控制的核心模块
- **主要功能**：
  - 机械臂归零
  - 关节控制（单个关节或多个关节同时控制）
  - 坐标控制（移动到指定XY坐标）
  - 预设动作（摇头、点头、跳舞等）
  - 视觉辅助（移动到俯视姿态、拍摄俯视图）
  - 手眼标定（像素坐标转换为机械臂坐标）

#### utils_pump.py
- **功能**：控制机械臂吸泵的开关
- **主要功能**：
  - 开启吸泵（抓取物体）
  - 关闭吸泵（释放物体）

#### utils_led.py
- **功能**：控制机械臂LED灯的颜色
- **实现方式**：通过大语言模型理解颜色描述，将其转换为RGB值，然后控制LED灯

#### utils_drag_teaching.py
- **功能**：实现机械臂的拖动示教功能
- **主要功能**：
  - 录制动作：用户手动拖动机械臂，记录动作序列
  - 回放动作：复现录制的动作序列
  - 循环回放：重复执行录制的动作序列

### 感知系统相关

#### utils_camera.py
- **功能**：摄像头的初始化和图像捕获
- **主要功能**：开启摄像头，实时显示画面

#### utils_asr.py
- **功能**：录音和语音识别
- **主要功能**：
  - 录音：支持固定时长录音和自动录音（音量触发）
  - 语音识别：将语音转换为文本

#### utils_tts.py
- **功能**：语音合成和音频播放
- **主要功能**：
  - 语音合成：将文本转换为语音
  - 播放音频：播放wav格式的音频文件

#### utils_vlm_move.py
- **功能**：多模态大模型识别图像，控制机械臂移动物体
- **主要流程**：
  1. 机械臂归零
  2. 接收用户指令
  3. 拍摄俯视图
  4. 调用多模态大模型识别图像
  5. 手眼标定转换坐标
  6. 控制吸泵吸取并移动物体

### 工具检查模块

#### camera_check.py
- **功能**：独立的摄像头检查工具，用于测试摄像头功能

#### sound_check.py
- **功能**：检查声音相关功能，包括录音、语音识别、语音合成和音频播放

## 文件调用关系

### 调用关系图

```
┌────────────────┐     ┌────────────────┐     ┌────────────────┐
│   agent_go.py  │────▶│ utils_agent.py │────▶│  utils_llm.py  │
└────────────────┘     └────────────────┘     └────────────────┘
        │                      ▲                     │
        │                      │                     │
        ▼                      │                     ▼
┌────────────────┐     ┌────────────────┐     ┌────────────────┐
│utils_vlm_move.py│────▶│ utils_robot.py │────▶│ utils_led.py   │
└────────────────┘     └────────────────┘     └────────────────┘
        │                      ▲                     │
        │                      │                     │
        ▼                      │                     ▼
┌────────────────┐     ┌────────────────┐     ┌────────────────┐
│  utils_vlm.py  │────▶│ utils_pump.py  │     │  utils_asr.py  │
└────────────────┘     └────────────────┘     └────────────────┘
        │                      │                     │
        │                      │                     │
        ▼                      ▼                     ▼
┌────────────────┐     ┌────────────────┐     ┌────────────────┐
│  utils_tts.py  │     │utils_camera.py │     │utils_drag_tea. │
└────────────────┘     └────────────────┘     └────────────────┘
```

### 核心依赖关系

1. **主程序依赖**：agent_go.py依赖所有其他工具模块
2. **智能体依赖**：utils_agent.py依赖utils_llm.py
3. **机械臂控制依赖**：
   - utils_robot.py依赖utils_pump.py
   - utils_vlm_move.py依赖utils_robot.py、utils_vlm.py
   - utils_led.py依赖utils_robot.py、utils_llm.py
4. **感知系统依赖**：
   - utils_vlm.py依赖utils_tts.py
   - utils_asr.py依赖API_KEY.py
   - utils_tts.py依赖API_KEY.py

## 核心流程

### 1. 系统初始化

```
agent_go.py
└─── 导入所有工具模块
└─── pump_off()  # 关闭吸泵
└─── play_wav('asset/welcome.wav')  # 播放欢迎词
```

### 2. 用户指令处理

```
agent_go.py
└─── 接收用户指令（语音或键盘输入）
    ├─── 语音输入：record() → speech_recognition()
    └─── 键盘输入：直接获取
```

### 3. 智能体规划动作

```
agent_go.py
└─── agent_plan()
    └─── utils_agent.py
        └─── 调用大语言模型API
            └─── utils_llm.py
                └─── 返回动作序列
```

### 4. 多模态视觉定位（可选）

```
utils_vlm_move.py
└─── 拍摄俯视图：top_view_shot()
└─── 调用多模态大模型：QwenVL_api()
    └─── utils_vlm.py
        └─── 识别图像中的物体位置
└─── 后处理和可视化：post_processing_viz()
└─── 手眼标定转换：eye2hand()
    └─── utils_robot.py
```

### 5. 机械臂动作执行

```
agent_go.py
└─── 执行动作序列
    ├─── 机械臂控制：back_zero(), head_shake(), move_to_coords()等
    │   └─── utils_robot.py
    ├─── 吸泵控制：pump_on(), pump_off()
    │   └─── utils_pump.py
    ├─── LED灯控制：llm_led()
    │   └─── utils_led.py
    └─── 多模态运动：vlm_move()
        └─── utils_vlm_move.py
```

### 6. 结果反馈

```
agent_go.py
└─── 语音反馈：tts() → play_wav()
    └─── utils_tts.py
```

## 技术亮点与创新点

### 1. 多模态智能规划

项目结合了大语言模型和多模态视觉模型，实现了从自然语言指令到机械臂动作的智能转换。用户可以用自然语言描述任务，如"帮我把绿色方块放在篮球上"，系统能够理解并执行相应的动作。

### 2. 灵活的手眼标定

通过eye2hand函数实现了像素坐标到机械臂坐标的转换，使得系统能够根据图像中的物体位置，精确控制机械臂移动到指定位置。

### 3. 丰富的交互方式

支持多种交互方式，包括语音输入、键盘输入、图像输入等，用户可以根据需要选择最方便的交互方式。

### 4. 可视化与调试支持

提供了丰富的可视化功能，如图像识别结果标记、动作序列记录等，方便用户调试和优化系统。

## 应用场景

1. **物品分拣与搬运**：根据用户指令，将指定物品移动到目标位置
2. **辅助生活**：帮助用户完成简单的生活任务，如取物、放物等
3. **教育演示**：展示AI与机器人技术的结合，用于教育和科普
4. **实验研究**：作为具身智能体的研究平台，探索AI与机器人的交互

## 总结

该项目实现了一个功能完整的AI机械臂智能体，具备感知、理解和执行能力。通过结合大语言模型、多模态视觉模型、语音识别、语音合成等技术，构建了一个能够与环境交互的具身智能系统。项目代码结构清晰，模块划分合理，具有良好的可扩展性和可维护性，可以作为具身智能研究和应用的基础平台。